"""
Markdownå†…å®¹å¤„ç†è„šæœ¬  - ç®€åŒ–ç‰ˆ

åªæå–æœ€é‡è¦çš„ä¿¡æ¯ï¼š
1. è¯¾ç¨‹åç§°å’Œè¯¾ç¨‹ä»£ç ï¼ˆAIæå–ï¼‰
2. è¯¾ç¨‹ç›®æ ‡ï¼ˆä»£ç å®šä½ + AIæ¸…ç†ï¼‰
3. æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»ï¼ˆä»£ç å®šä½ + AIè§£æï¼‰

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-01-07
"""

import os
import re
import json
import time
import requests
from datetime import datetime
from typing import Dict, List, Optional

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: ollamaåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ollama")

# DeepSeek APIé…ç½®
DEEPSEEK_API_KEY = "sk-07210ded9e714b96befe824d8c79fde4"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"


# ==================== ä»»åŠ¡1: æå–è¯¾ç¨‹åç§°å’Œä»£ç  (ä»£ç ä¼˜å…ˆ+AIè¾…åŠ©) ====================

def extract_course_code_with_regex(md_content: str) -> Optional[str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¯¾ç¨‹ä»£ç 

    Args:
        md_content: Markdownæ–‡ä»¶å†…å®¹

    Returns:
        è¯¾ç¨‹ä»£ç ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    # æ¨¡å¼1: **A2301210** (åŠ ç²—ï¼Œæ ‡å‡†æ ¼å¼ï¼š1ä¸ªå¤§å†™å­—æ¯+7ä½æ•°å­—)
    pattern1 = r'\*\*([A-Z]\d{7})\*\*'
    match = re.search(pattern1, md_content)
    if match:
        return match.group(1)

    # æ¨¡å¼2: |è¯¾ç¨‹ä»£ç |A2301210| (è¡¨æ ¼ä¸­ï¼Œæ ‡å‡†æ ¼å¼)
    pattern2 = r'\|è¯¾ç¨‹(?:ä»£ç |ç¼–å·)\|([A-Z]\d{7})\|'
    match = re.search(pattern2, md_content)
    if match:
        return match.group(1)

    # æ¨¡å¼3: **A051201s** (åŠ ç²—ï¼Œéæ ‡å‡†æ ¼å¼ï¼šå¯èƒ½æœ‰å°å†™å­—æ¯åç¼€)
    pattern3 = r'\*\*([A-Z][A-Za-z0-9]{6,8})\*\*'
    match = re.search(pattern3, md_content)
    if match:
        code = match.group(1)
        # éªŒè¯è‡³å°‘æœ‰6ä¸ªå­—ç¬¦
        if len(code) >= 7:
            return code

    # æ¨¡å¼4: |è¯¾ç¨‹ä»£ç |A051201s| (è¡¨æ ¼ä¸­ï¼Œéæ ‡å‡†æ ¼å¼)
    pattern4 = r'\|è¯¾ç¨‹(?:ä»£ç |ç¼–å·)\|([A-Z][A-Za-z0-9]{6,8})\|'
    match = re.search(pattern4, md_content)
    if match:
        code = match.group(1)
        if len(code) >= 7:
            return code

    return None


def extract_course_name_with_regex(md_content: str) -> Optional[str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¯¾ç¨‹åç§°

    Args:
        md_content: Markdownæ–‡ä»¶å†…å®¹

    Returns:
        è¯¾ç¨‹åç§°ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    # æ¨¡å¼1: # ã€Šè¯¾ç¨‹åã€‹è¯¾ç¨‹æ•™å­¦å¤§çº²
    pattern1 = r'#\s*ã€Š(.*?)ã€‹'
    match = re.search(pattern1, md_content)
    if match:
        return match.group(1).strip()

    # æ¨¡å¼2: ## ã€Šè¯¾ç¨‹åã€‹è¯¾ç¨‹æ•™å­¦å¤§çº²
    pattern2 = r'##\s*ã€Š(.*?)ã€‹'
    match = re.search(pattern2, md_content)
    if match:
        return match.group(1).strip()

    return None


def extract_course_basic_info_with_ai(md_content: str, model: str = 'qwen2.5:7b') -> Dict[str, str]:
    """
    ä½¿ç”¨AIæå–è¯¾ç¨‹åç§°å’Œè¯¾ç¨‹ä»£ç 
    
    Args:
        md_content: Markdownæ–‡ä»¶å†…å®¹
        model: ä½¿ç”¨çš„AIæ¨¡å‹
        
    Returns:
        åŒ…å«course_nameå’Œcourse_codeçš„å­—å…¸
    """
    # åªæå–å‰800è¡Œï¼ŒåŒ…å«è¯¾ç¨‹åŸºæœ¬ä¿¡æ¯
    lines = md_content.split('\n')[:800]
    header_text = '\n'.join(lines)
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯¾ç¨‹ä¿¡æ¯æå–åŠ©æ‰‹ã€‚

ä»»åŠ¡ï¼šä»è¯¾ç¨‹æ•™å­¦å¤§çº²çš„Markdownæ–‡æœ¬ä¸­æå–è¯¾ç¨‹åç§°å’Œè¯¾ç¨‹ä»£ç ã€‚

æå–è§„åˆ™ï¼š
1. è¯¾ç¨‹åç§°ï¼š
   - é€šå¸¸åœ¨æ ‡é¢˜ä¸­ï¼Œæ ¼å¼å¦‚ï¼š# ã€Šè¯¾ç¨‹åã€‹è¯¾ç¨‹æ•™å­¦å¤§çº² æˆ– ## ã€Šè¯¾ç¨‹åã€‹è¯¾ç¨‹æ•™å­¦å¤§çº²
   - æå–ã€Šã€‹ä¸­çš„å†…å®¹ï¼Œ**å¿…é¡»ä¿ç•™å®Œæ•´çš„è¯¾ç¨‹åç§°ï¼ŒåŒ…æ‹¬æ•°å­—ã€ç©ºæ ¼ã€æ‹¬å·ç­‰æ‰€æœ‰å­—ç¬¦**
   - ä¾‹å¦‚ï¼š"ã€Šå¤§å­¦ç‰©ç† 1ã€‹" â†’ "å¤§å­¦ç‰©ç† 1"ï¼ˆä¿ç•™ç©ºæ ¼å’Œæ•°å­—ï¼‰
   - ä¾‹å¦‚ï¼š"ã€Šå¤§å­¦ç‰©ç†2ã€‹" â†’ "å¤§å­¦ç‰©ç†2"ï¼ˆä¿ç•™æ•°å­—ï¼‰
   - ä¾‹å¦‚ï¼š"ã€Šç‰©ç†å­¦åŸç†åŠå·¥ç¨‹åº”ç”¨1ã€‹" â†’ "ç‰©ç†å­¦åŸç†åŠå·¥ç¨‹åº”ç”¨1"ï¼ˆä¿ç•™æ•°å­—ï¼‰
   - å¦‚æœæ²¡æœ‰ã€Šã€‹ï¼Œæå–æ ‡é¢˜ä¸­çš„è¯¾ç¨‹åç§°éƒ¨åˆ†
   - ä¸è¦åŒ…å«"è¯¾ç¨‹æ•™å­¦å¤§çº²"è¿™å‡ ä¸ªå­—
   - **ä¸è¦åˆ é™¤æˆ–ä¿®æ”¹è¯¾ç¨‹åç§°ä¸­çš„ä»»ä½•å­—ç¬¦**

2. è¯¾ç¨‹ä»£ç ï¼š
   - æ ‡å‡†æ ¼å¼ï¼š1ä¸ªå¤§å†™å­—æ¯ + 7ä½æ•°å­—ï¼Œå¦‚ï¼šA2301210ã€S0718060ã€A0501180
   - éæ ‡å‡†æ ¼å¼ï¼šå¯èƒ½æœ‰å°å†™å­—æ¯åç¼€ï¼Œå¦‚ï¼šA051201sã€ETH01
   - åœ¨è¡¨æ ¼ä¸­ï¼Œæ ‡æ³¨ä¸º"è¯¾ç¨‹ç¼–å·"ã€"è¯¾ç¨‹ä»£ç "ã€"è¯¾ç¨‹ä»£ç "ç­‰
   - é€šå¸¸ç”¨**åŠ ç²—**æ ‡è®°ï¼Œå¦‚ï¼š**A2301210**ã€**A051201s**
   - æ³¨æ„ï¼šä¸æ˜¯å­¦åˆ†ã€å­¦æ—¶ç­‰å…¶ä»–æ•°å­—
   - é•¿åº¦é€šå¸¸åœ¨7-9ä¸ªå­—ç¬¦ä¹‹é—´

ç¤ºä¾‹1ï¼š
è¾“å…¥ï¼š|è¯¾ç¨‹ä»£ç |**A2301210**|è¯¾ç¨‹ç±»åˆ«|é€šè¯†å…¬å…±è¯¾|
è¾“å‡ºï¼š{"course_name": "...", "course_code": "A2301210"}

ç¤ºä¾‹2ï¼š
è¾“å…¥ï¼š|è¯¾ç¨‹ä»£ç |**A051201s**|è¯¾ç¨‹ç±»åˆ«|å­¦ç§‘åŸºç¡€è¯¾|
è¾“å‡ºï¼š{"course_name": "...", "course_code": "A051201s"}

é‡è¦ï¼š
- åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹
- ä¸è¦è¾“å‡ºè¯¾ç¨‹ç›®æ ‡ã€å­¦æ—¶åˆ†é…ç­‰å…¶ä»–ä¿¡æ¯
- åªè¾“å‡ºcourse_nameå’Œcourse_codeä¸¤ä¸ªå­—æ®µ
- æ¥å—éæ ‡å‡†æ ¼å¼çš„è¯¾ç¨‹ä»£ç ï¼ˆå¦‚å¸¦å°å†™å­—æ¯åç¼€ï¼‰

è¾“å‡ºæ ¼å¼ï¼š
{
  "course_name": "è¯¾ç¨‹åç§°",
  "course_code": "è¯¾ç¨‹ä»£ç "
}

å¦‚æœæ‰¾ä¸åˆ°ï¼Œå¯¹åº”å­—æ®µå¡«å†™"æœªæ‰¾åˆ°"ã€‚
"""
    
    user_prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–è¯¾ç¨‹åç§°å’Œè¯¾ç¨‹ä»£ç ï¼š

{header_text}

åªè¾“å‡ºJSONï¼Œæ ¼å¼ï¼š{{"course_name": "...", "course_code": "..."}}"""
    
    try:
        response = ollama.chat(
            model=model,
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ],
            options={
                'temperature': 0.1,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§çš„è¾“å‡º
            }
        )
        
        result_text = response['message']['content'].strip()
        
        # å°è¯•è§£æJSON
        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        result_text = re.sub(r'```json\s*', '', result_text)
        result_text = re.sub(r'```\s*', '', result_text)
        
        result = json.loads(result_text)
        
        return {
            'course_name': result.get('course_name', 'æœªæ‰¾åˆ°'),
            'course_code': result.get('course_code', 'æœªæ‰¾åˆ°')
        }
        
    except json.JSONDecodeError as e:
        print(f"      âš ï¸  JSONè§£æå¤±è´¥: {e}")
        print(f"      åŸå§‹å“åº”: {result_text[:200]}...")
        return {
            'course_name': 'æœªæ‰¾åˆ°',
            'course_code': 'æœªæ‰¾åˆ°',
            'raw_response': result_text
        }
    except Exception as e:
        print(f"      âš ï¸  AIè°ƒç”¨å¤±è´¥: {e}")
        return {
            'course_name': 'æœªæ‰¾åˆ°',
            'course_code': 'æœªæ‰¾åˆ°',
            'error': str(e)
        }


# ==================== ä»»åŠ¡2: æå–å¹¶æ¸…ç†è¯¾ç¨‹ç›®æ ‡ (ä»£ç +AI) ====================

def extract_chapter_1_raw(md_content: str) -> Optional[str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å®šä½"ä¸€ã€è¯¾ç¨‹ç›®æ ‡"ç« èŠ‚

    Args:
        md_content: Markdownæ–‡ä»¶å†…å®¹

    Returns:
        ç¬¬ä¸€ç« çš„åŸå§‹å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    # åŒ¹é…ä»"ä¸€ã€ è¯¾ç¨‹ç›®æ ‡"åˆ°"äºŒã€"ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
    # å¢åŠ æ›´å¤šæˆªæ–­æ¡ä»¶ï¼š
    # - "äºŒã€" - æ ‡å‡†çš„ç¬¬äºŒç« 
    # - "ä¸‰ã€" - å¦‚æœæ²¡æœ‰ç¬¬äºŒç« ï¼Œå¯èƒ½ç›´æ¥åˆ°ç¬¬ä¸‰ç« 
    # - "è¯¾\n\nç¨‹å†…å®¹ä¸åŸºæœ¬è¦æ±‚" - PDFè½¬æ¢æ—¶æ–­è¡Œçš„"è¯¾ç¨‹å†…å®¹ä¸åŸºæœ¬è¦æ±‚"
    # - "æ•™å­¦å†…å®¹" - æ•™å­¦å†…å®¹ç« èŠ‚
    # - "æ•™å­¦æ–¹æ³•" - æ•™å­¦æ–¹æ³•ç« èŠ‚
    pattern = r'(ä¸€ã€\s*è¯¾ç¨‹ç›®æ ‡[\s\S]*?)(?=äºŒã€|ä¸‰ã€|è¯¾\s*\n+\s*ç¨‹å†…å®¹ä¸åŸºæœ¬è¦æ±‚|æ•™å­¦å†…å®¹|æ•™å­¦æ–¹æ³•|$)'
    match = re.search(pattern, md_content)

    if match:
        return match.group(1).strip()
    return None


def clean_chapter_1_with_ai(raw_text: str, model: str = 'qwen2.5:7b') -> Dict:
    """
    ä½¿ç”¨AIæ¸…ç†è¯¾ç¨‹ç›®æ ‡æ–‡æœ¬å¹¶ç»“æ„åŒ–

    Args:
        raw_text: åŸå§‹çš„è¯¾ç¨‹ç›®æ ‡æ–‡æœ¬
        model: ä½¿ç”¨çš„AIæ¨¡å‹

    Returns:
        ç»“æ„åŒ–çš„è¯¾ç¨‹ç›®æ ‡æ•°æ®
    """
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ¸…ç†å’Œç»“æ„åŒ–åŠ©æ‰‹ã€‚

ä»»åŠ¡ï¼šå°†è¯¾ç¨‹ç›®æ ‡æ–‡æœ¬æ¸…ç†å¹¶ç»“æ„åŒ–ä¸ºJSONæ ¼å¼ã€‚

æ¸…ç†å’Œæå–è§„åˆ™ï¼š
1. æå–æ€»è¿°ï¼ˆoverviewï¼‰ï¼š
   - æå–"ä¸€ã€è¯¾ç¨‹ç›®æ ‡"æ ‡é¢˜åã€ç¬¬ä¸€ä¸ªå…·ä½“è¯¾ç¨‹ç›®æ ‡ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
   - åˆ é™¤é¡µç ï¼ˆå¦‚ "534"ã€"721"ï¼‰
   - åˆå¹¶è¢«æ‰“æ–­çš„å¥å­
   - åˆ é™¤å¤šä½™ç©ºè¡Œ
   - åŒ…æ‹¬è¯¾ç¨‹èƒŒæ™¯ã€æ„ä¹‰ã€"æœ¬è¯¾ç¨‹æ‹Ÿé€šè¿‡æ•™å­¦æ´»åŠ¨ï¼Œè¾¾åˆ°ä»¥ä¸‹è¯¾ç¨‹ç›®æ ‡ï¼š"ç­‰å¼•å¯¼æ€§æ–‡å­—

2. æå–å…·ä½“è¯¾ç¨‹ç›®æ ‡ï¼ˆgoalsï¼‰ï¼š
   - è¯†åˆ«æ‰€æœ‰ç¼–å·çš„è¯¾ç¨‹ç›®æ ‡ï¼ˆå¦‚"è¯¾ç¨‹ç›®æ ‡1"ã€"è¯¾ç¨‹ç›®æ ‡ **1**"ã€"1."ç­‰ï¼‰
   - æå–ç›®æ ‡ç¼–å·ï¼ˆnumberï¼‰ï¼šåªæå–æ•°å­—ï¼Œå¦‚"1"ã€"2"ã€"3"
   - æå–ç›®æ ‡å†…å®¹ï¼ˆcontentï¼‰ï¼šæå–å†’å·æˆ–å¥å·åçš„å®Œæ•´æè¿°
   - åˆ é™¤é¡µç ã€åˆå¹¶è¢«æ‰“æ–­çš„å¥å­ã€åˆ é™¤å¤šä½™ç©ºè¡Œ
   - ä¿ç•™æ‰€æœ‰å®è´¨æ€§å†…å®¹ï¼Œä¸è¦æ€»ç»“æˆ–åˆ å‡

è¾“å‡ºæ ¼å¼ï¼š
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼š
{
  "overview": "è¯¾ç¨‹ç›®æ ‡çš„æ€»è¿°éƒ¨åˆ†",
  "goals": [
    {
      "number": "1",
      "content": "ç¬¬ä¸€ä¸ªè¯¾ç¨‹ç›®æ ‡çš„å®Œæ•´å†…å®¹"
    },
    {
      "number": "2",
      "content": "ç¬¬äºŒä¸ªè¯¾ç¨‹ç›®æ ‡çš„å®Œæ•´å†…å®¹"
    }
  ]
}

æ³¨æ„ï¼š
- åªè¾“å‡ºJSONï¼Œä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹
- å¦‚æœæ²¡æœ‰æ€»è¿°éƒ¨åˆ†ï¼Œoverviewä¸ºç©ºå­—ç¬¦ä¸²
- å¦‚æœæ²¡æœ‰å…·ä½“ç›®æ ‡ï¼Œgoalsä¸ºç©ºæ•°ç»„
"""

    user_prompt = f"""è¯·æ¸…ç†å¹¶ç»“æ„åŒ–ä»¥ä¸‹è¯¾ç¨‹ç›®æ ‡æ–‡æœ¬ï¼š

{raw_text}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"""

    try:
        response = ollama.chat(
            model=model,
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ],
            options={
                'temperature': 0.1,
            }
        )

        result_text = response['message']['content'].strip()

        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        result_text = re.sub(r'```json\s*', '', result_text)
        result_text = re.sub(r'```\s*', '', result_text)

        result = json.loads(result_text)
        return result

    except json.JSONDecodeError as e:
        print(f"      âš ï¸  JSONè§£æå¤±è´¥")
        # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºoverview
        return {
            'overview': raw_text,
            'goals': []
        }
    except Exception as e:
        print(f"      âš ï¸  AIæ¸…ç†å¤±è´¥: {e}")
        # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºoverview
        return {
            'overview': raw_text,
            'goals': []
        }


# ==================== ä»»åŠ¡3: æå–æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³» (ä»£ç +AI) ====================

def extract_chapter_2_raw(md_content: str) -> Optional[str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å®šä½"äºŒã€..."ç« èŠ‚
    
    Args:
        md_content: Markdownæ–‡ä»¶å†…å®¹
        
    Returns:
        ç¬¬äºŒç« çš„åŸå§‹å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    # åŒ¹é…ä»"äºŒã€"åˆ°"ä¸‰ã€"ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
    pattern = r'(äºŒã€[\s\S]*?)(?=ä¸‰ã€|$)'
    match = re.search(pattern, md_content)
    
    if match:
        return match.group(1).strip()
    return None


def validate_chapter_2_title(chapter_2_text: str) -> bool:
    """
    éªŒè¯ç¬¬äºŒç« æ˜¯å¦æ˜¯å…³äº"è¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»"ï¼Œå¹¶ä¸”åŒ…å«å®é™…å†…å®¹

    Args:
        chapter_2_text: ç¬¬äºŒç« çš„æ–‡æœ¬

    Returns:
        æ˜¯å¦æ˜¯å¯¹åº”å…³ç³»ç« èŠ‚ä¸”åŒ…å«æœ‰æ•ˆå†…å®¹
    """
    # æ­¥éª¤1ï¼šæ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
    first_line = chapter_2_text.split('\n')[0]
    keywords = ['è¯¾ç¨‹ç›®æ ‡', 'æ¯•ä¸šè¦æ±‚', 'å¯¹åº”å…³ç³»']
    if not all(keyword in first_line for keyword in keywords):
        return False

    # æ­¥éª¤2ï¼šæ£€æŸ¥å†…å®¹æ˜¯å¦æ˜ç¡®è¯´æ˜"ä¸åšæè¿°"æˆ–"æ— "
    # è¿™äº›è¯¾ç¨‹è™½ç„¶æœ‰æ ‡é¢˜ï¼Œä½†å®é™…ä¸Šæ²¡æœ‰å¯¹åº”å…³ç³»è¡¨æ ¼
    skip_patterns = [
        'ä¸åšæè¿°',
        'æ­¤ä¸åšæè¿°',
        'ä¸ä½œæè¿°',
        'å„ä¸“ä¸šæ¯•ä¸šè¦æ±‚å„å¼‚',
        'å› å„ä¸“ä¸š.*ä¸åšæè¿°',
        'æš‚æ— ',
        '^æ— $',
        '^æ— ã€‚$',
    ]

    # è·å–æ ‡é¢˜åçš„å†…å®¹ï¼ˆå‰200ä¸ªå­—ç¬¦è¶³å¤Ÿåˆ¤æ–­ï¼‰
    content_after_title = '\n'.join(chapter_2_text.split('\n')[1:])[:200]

    for pattern in skip_patterns:
        if re.search(pattern, content_after_title):
            return False

    # æ­¥éª¤3ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æ ‡è®°ï¼ˆ|ç¬¦å·ï¼‰
    # å¦‚æœæ ‡é¢˜æ­£ç¡®ä½†å®Œå…¨æ²¡æœ‰è¡¨æ ¼ï¼Œä¹Ÿåº”è¯¥è·³è¿‡
    if '|' not in chapter_2_text:
        return False

    return True


def extract_tables_raw(chapter_2_text: str) -> List[str]:
    """
    ä»ç¬¬äºŒç« ä¸­æå–æ‰€æœ‰è¡¨æ ¼

    Args:
        chapter_2_text: ç¬¬äºŒç« çš„æ–‡æœ¬

    Returns:
        è¡¨æ ¼æ–‡æœ¬åˆ—è¡¨
    """
    tables = []

    # æ¨¡å¼1: åŒ¹é…ä»¥"è¡¨X"æˆ–"è¡¨ X"å¼€å¤´çš„è¡¨æ ¼ï¼ˆåŒ…æ‹¬"è¡¨ **1-1**"è¿™ç§æ ¼å¼ï¼‰
    # åŒ¹é…ä»"è¡¨"å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ª"è¡¨"æˆ–"ä¸‰ã€"æˆ–æ–‡ä»¶ç»“å°¾
    # æ³¨æ„ï¼šè¿™é‡Œè¦åŒ¹é…"è¡¨"å¼€å¤´çš„è¡Œï¼ŒåŒ…æ‹¬å‰é¢å¯èƒ½æœ‰çš„æ–‡å­—ï¼ˆå¦‚"å¦‚è¡¨ 1 æ‰€ç¤º"ï¼‰
    pattern1 = r'((?:.*è¡¨\s*[\*\d\-]+.*\n)[\s\S]*?\|[\s\S]*?)(?=\n\n+.*è¡¨\s*[\*\d\-]+|\n\n+ä¸‰ã€|\Z)'
    matches1 = re.finditer(pattern1, chapter_2_text)

    for match in matches1:
        table_text = match.group(1).strip()
        # ç¡®ä¿æ˜¯è¡¨æ ¼ï¼ˆåŒ…å«|ç¬¦å·ï¼‰ä¸”åŒ…å«"æ¯•ä¸šè¦æ±‚"å…³é”®è¯
        if '|' in table_text and ('æ¯•ä¸šè¦æ±‚' in table_text or 'æŒ‡æ ‡ç‚¹' in table_text or 'è¯¾ç¨‹ç›®æ ‡' in table_text):
            tables.append(table_text)

    return tables


def preprocess_table(table_text: str) -> str:
    """
    é¢„å¤„ç†è¡¨æ ¼ï¼Œä¿®å¤å¼‚å¸¸æ ¼å¼

    é—®é¢˜1ï¼šæœ‰äº›è¡¨æ ¼çš„æ ‡é¢˜è¢«åµŒå…¥åˆ°è¡¨æ ¼çš„ç¬¬ä¸€è¡Œä¸­ï¼Œä¾‹å¦‚ï¼š
    è¡¨

    1 æ‰€ç¤ºã€‚

    |è¡¨1 è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ä¸“ä¸šè¯¾ç¨‹|ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»|Col3|
    |---|---|---|
    |æ¯•ä¸šè¦æ±‚|æŒ‡æ ‡ç‚¹|è¯¾ç¨‹ç›®æ ‡|

    é—®é¢˜2ï¼šæœ‰äº›è¡¨æ ¼çš„æ ‡é¢˜è¢«æ–­æˆå¤šè¡Œï¼Œä¾‹å¦‚ï¼š
    è¡¨ 5 æ™ºèƒ½ç¡¬ä»¶ä¸ç³»ç»Ÿä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹


    åº”å…³ç³»

    è§£å†³ï¼š
    1. æ£€æµ‹å¹¶åˆå¹¶æ–­è¡Œçš„æ ‡é¢˜
    2. æ£€æµ‹å¹¶æå–åµŒå…¥è¡¨æ ¼çš„æ ‡é¢˜
    3. é‡æ„è¡¨æ ¼ï¼Œåˆ é™¤æ— å…³æ–‡å­—

    Args:
        table_text: åŸå§‹è¡¨æ ¼æ–‡æœ¬

    Returns:
        é¢„å¤„ç†åçš„è¡¨æ ¼æ–‡æœ¬
    """
    lines = table_text.split('\n')

    if len(lines) < 3:
        return table_text

    # ============================================================
    # æ­¥éª¤1ï¼šåˆå¹¶æ–­è¡Œçš„æ ‡é¢˜
    # ============================================================
    # æ£€æµ‹ç‰¹å¾ï¼š
    # - æŸä¸€è¡Œä»¥"å¯¹"ã€"å…³"ã€"ç³»"ç­‰å­—ç»“å°¾ï¼ˆæ ‡é¢˜çš„ä¸€éƒ¨åˆ†ï¼‰
    # - åé¢æœ‰è‹¥å¹²ç©ºè¡Œ
    # - å†åé¢æœ‰ä¸€è¡Œä»¥"åº”"ã€"ç³»"ã€"è¡¨"ç­‰å­—å¼€å¤´ï¼ˆæ ‡é¢˜çš„å¦ä¸€éƒ¨åˆ†ï¼‰
    #
    # ç¤ºä¾‹ï¼š
    # "...æ¯•ä¸šè¦æ±‚å¯¹\n\n\nåº”å…³ç³»" â†’ "...æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»"
    # "...è¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹\n\nåº”å…³ç³»" â†’ "...è¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»"

    # å¸¸è§çš„æ–­è¡Œæ¨¡å¼
    title_break_patterns = [
        (r'å¯¹$', r'^åº”'),      # "å¯¹" + "åº”" â†’ "å¯¹åº”"
        (r'å…³$', r'^ç³»'),      # "å…³" + "ç³»" â†’ "å…³ç³»"
        (r'å¯¹$', r'^åº”å…³ç³»'),  # "å¯¹" + "åº”å…³ç³»" â†’ "å¯¹åº”å…³ç³»"
        (r'è¦æ±‚$', r'^å¯¹åº”'),  # "è¦æ±‚" + "å¯¹åº”" â†’ "è¦æ±‚å¯¹åº”"
    ]

    # å°è¯•åˆå¹¶æ–­è¡Œ
    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # è·³è¿‡ç©ºè¡Œå’Œè¡¨æ ¼è¡Œ
        if not line or line.startswith('|'):
            i += 1
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ…å«"è¡¨"å­—ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰
        if 'è¡¨' in line and ('ä¸“ä¸š' in line or 'è¯¾ç¨‹' in line or 'æ¯•ä¸šè¦æ±‚' in line):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ–­è¡Œæ¨¡å¼
            for end_pattern, start_pattern in title_break_patterns:
                if re.search(end_pattern, line):
                    # æŸ¥æ‰¾åç»­çš„éç©ºè¡Œï¼ˆè·³è¿‡ç©ºè¡Œï¼‰
                    j = i + 1
                    while j < len(lines) and not lines[j].strip():
                        j += 1

                    if j < len(lines):
                        next_line = lines[j].strip()
                        # æ£€æŸ¥ä¸‹ä¸€ä¸ªéç©ºè¡Œæ˜¯å¦åŒ¹é…å¼€å§‹æ¨¡å¼
                        if re.search(start_pattern, next_line) and not next_line.startswith('|'):
                            # æ‰¾åˆ°æ–­è¡Œï¼åˆå¹¶å®ƒä»¬
                            merged_line = line + next_line

                            # æ›¿æ¢åŸæ¥çš„è¡Œ
                            lines[i] = merged_line

                            # åˆ é™¤ä¸­é—´çš„ç©ºè¡Œå’Œä¸‹ä¸€è¡Œ
                            del lines[i+1:j+1]

                            # ä¸å¢åŠ iï¼Œç»§ç»­æ£€æŸ¥å½“å‰è¡Œï¼ˆå¯èƒ½æœ‰å¤šæ¬¡æ–­è¡Œï¼‰
                            break
            else:
                # æ²¡æœ‰åŒ¹é…ä»»ä½•æ¨¡å¼ï¼Œç»§ç»­ä¸‹ä¸€è¡Œ
                i += 1
        else:
            i += 1

    # é‡æ–°ç»„åˆæ–‡æœ¬
    table_text = '\n'.join(lines)

    # ============================================================
    # æ­¥éª¤2ï¼šæŸ¥æ‰¾ç‹¬ç«‹çš„æ ‡é¢˜è¡Œï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    # ============================================================
    # åœ¨è¡¨æ ¼å‰æŸ¥æ‰¾ç‹¬ç«‹çš„æ ‡é¢˜è¡Œï¼Œä¾‹å¦‚ï¼š
    # "è¡¨ **3** è¯¾ç¨‹ç›®æ ‡ä¸è®¡ç®—æœºç§‘å­¦è‹±æ‰ç­ï¼ˆè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼‰ä¸“ä¸šæ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»"

    independent_title = None
    independent_title_idx = -1

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¡¨æ ¼è¡Œï¼ˆä»¥|å¼€å¤´ï¼‰
    first_table_line_idx = -1
    for i, line in enumerate(lines):
        if line.strip().startswith('|'):
            first_table_line_idx = i
            break

    if first_table_line_idx == -1:
        return table_text

    # åœ¨è¡¨æ ¼å‰æŸ¥æ‰¾ç‹¬ç«‹çš„æ ‡é¢˜è¡Œ
    # ç‰¹å¾ï¼šåŒ…å«"è¡¨"ã€"è¯¾ç¨‹ç›®æ ‡"ã€"ä¸“ä¸š"ã€"æ¯•ä¸šè¦æ±‚"ã€"å¯¹åº”å…³ç³»"
    for i in range(max(0, first_table_line_idx - 10), first_table_line_idx):
        line = lines[i].strip()
        if (line and
            not line.startswith('|') and
            'è¡¨' in line and
            'è¯¾ç¨‹ç›®æ ‡' in line and
            ('ä¸“ä¸š' in line or 'æ¯•ä¸šè¦æ±‚' in line) and
            'å¯¹åº”å…³ç³»' in line):
            # æ‰¾åˆ°ç‹¬ç«‹çš„æ ‡é¢˜è¡Œ
            independent_title = line
            independent_title_idx = i
            break

    # ============================================================
    # æ­¥éª¤3ï¼šå¤„ç†æ ‡é¢˜åµŒå…¥è¡¨æ ¼ç¬¬ä¸€è¡Œçš„æƒ…å†µ
    # ============================================================

    # æå–ç¬¬ä¸€ä¸ªè¡¨æ ¼è¡Œçš„å•å…ƒæ ¼
    first_table_line = lines[first_table_line_idx].strip()
    cells = [cell.strip() for cell in first_table_line.split('|') if cell.strip()]

    # æ£€æŸ¥ç¬¬ä¸€ä¸ªå•å…ƒæ ¼æ˜¯å¦åŒ…å«"è¡¨"å­—ï¼ˆè¡¨æ ¼æ ‡é¢˜çš„ç‰¹å¾ï¼‰
    if len(cells) >= 2 and 'è¡¨' in cells[0] and ('ä¸“ä¸š' in cells[0] or 'è¯¾ç¨‹' in cells[0]):
        # è¿™æ˜¯ä¸€ä¸ªå¼‚å¸¸æ ¼å¼çš„è¡¨æ ¼ï¼Œæ ‡é¢˜åœ¨è¡¨æ ¼çš„ç¬¬ä¸€è¡Œ

        # å¦‚æœæ‰¾åˆ°äº†ç‹¬ç«‹çš„æ ‡é¢˜è¡Œï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
        if independent_title:
            title = independent_title
        else:
            # æ²¡æœ‰ç‹¬ç«‹æ ‡é¢˜ï¼Œä»è¡¨æ ¼ç¬¬ä¸€è¡Œæå–
            # è¿‡æ»¤æ‰æ— æ„ä¹‰çš„åˆ—ï¼ˆå¦‚"Col3"ã€"Col2"ç­‰ï¼‰
            meaningful_cells = []
            for cell in cells:
                # è·³è¿‡"ColX"è¿™ç§åˆ—
                if re.match(r'^Col\d+$', cell, re.IGNORECASE):
                    continue
                meaningful_cells.append(cell)

            # åˆå¹¶æœ‰æ„ä¹‰çš„å•å…ƒæ ¼ä½œä¸ºæ ‡é¢˜
            # æ™ºèƒ½åˆå¹¶ï¼šå¦‚æœå‰ä¸€ä¸ªå•å…ƒæ ¼ä»¥æŸä¸ªå­—ç»“å°¾ï¼Œåä¸€ä¸ªå•å…ƒæ ¼ä»¥åŒä¸€ä¸ªå­—å¼€å¤´ï¼Œåˆ™å»æ‰é‡å¤
            title_parts = []
            for i, cell in enumerate(meaningful_cells):
                if i == 0:
                    title_parts.append(cell)
                else:
                    prev_cell = title_parts[-1]
                    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤å­—ç¬¦ï¼ˆå¦‚"è¯¾ç¨‹" + "ç¨‹ç›®æ ‡" â†’ "è¯¾ç¨‹ç›®æ ‡"ï¼‰
                    merged = False
                    for overlap_len in range(min(3, len(prev_cell), len(cell)), 0, -1):
                        if prev_cell[-overlap_len:] == cell[:overlap_len]:
                            # æœ‰é‡å¤ï¼Œåˆå¹¶æ—¶å»æ‰é‡å¤éƒ¨åˆ†
                            title_parts[-1] = prev_cell + cell[overlap_len:]
                            merged = True
                            break
                    if not merged:
                        title_parts.append(cell)

            title = ''.join(title_parts)

        # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯åˆ†éš”ç¬¦è¡Œï¼ˆ---|---|---ï¼‰
        if first_table_line_idx + 1 < len(lines) and '---' in lines[first_table_line_idx + 1]:
            # åˆ é™¤æ ‡é¢˜è¡Œå’Œåˆ†éš”ç¬¦è¡Œï¼Œä¿ç•™è¡¨å¤´å’Œæ•°æ®è¡Œ
            #
            # è¡¨æ ¼ç»“æ„ï¼š
            # first_table_line_idx:     |è¡¨1 xxx|xxx|Col3|  <- æ ‡é¢˜è¡Œï¼ˆè¦åˆ é™¤ï¼‰
            # first_table_line_idx + 1: |---|---|---|       <- åˆ†éš”ç¬¦è¡Œï¼ˆè¦åˆ é™¤ï¼‰
            # first_table_line_idx + 2: |æ¯•ä¸šè¦æ±‚|æŒ‡æ ‡ç‚¹|è¯¾ç¨‹ç›®æ ‡|  <- è¡¨å¤´è¡Œï¼ˆè¦ä¿ç•™ï¼‰
            # first_table_line_idx + 3: |æ•°æ®1|æ•°æ®2|æ•°æ®3|  <- æ•°æ®è¡Œï¼ˆè¦ä¿ç•™ï¼‰

            # ä»è¡¨å¤´è¡Œå¼€å§‹ä¿ç•™ï¼ˆfirst_table_line_idx + 2ï¼‰
            remaining_table = lines[first_table_line_idx + 2:]

            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€ä¸ªè¡¨æ ¼ï¼ˆä»¥|è¡¨Xå¼€å¤´çš„è¡Œï¼‰
            # å¦‚æœæœ‰ï¼Œåœ¨é‚£é‡Œæˆªæ–­
            table_end_idx = len(remaining_table)
            for i, line in enumerate(remaining_table):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¦ä¸€ä¸ªè¡¨æ ¼çš„æ ‡é¢˜è¡Œ
                # ç‰¹å¾ï¼šä»¥|å¼€å¤´ï¼ŒåŒ…å«"è¡¨"å­—ï¼ŒåŒ…å«"ä¸“ä¸š"æˆ–"è¯¾ç¨‹ç›®æ ‡"ï¼ˆå®Œæ•´è¯ï¼‰ï¼Œä¸”åŒ…å«"å¯¹åº”å…³ç³»"
                # æ’é™¤è¡¨å¤´è¡Œï¼ˆå¦‚"|æ¯•ä¸šè¦æ±‚|æŒ‡æ ‡ç‚¹|è¯¾ç¨‹ç›®æ ‡|"ï¼‰
                if (line.strip().startswith('|') and
                    'è¡¨' in line and
                    ('ä¸“ä¸š' in line or 'è¯¾ç¨‹ç›®æ ‡' in line) and
                    'å¯¹åº”å…³ç³»' in line):
                    table_end_idx = i
                    break

            # åªä¿ç•™åˆ°ä¸‹ä¸€ä¸ªè¡¨æ ¼ä¹‹å‰çš„å†…å®¹
            remaining_table = remaining_table[:table_end_idx]

            # é‡æ–°æ„å»ºï¼šæ ‡é¢˜ + ç©ºè¡Œ + è¡¨å¤´å’Œæ•°æ®è¡Œ
            new_lines = [title, ''] + remaining_table
            new_table = '\n'.join(new_lines)

            return new_table

    return table_text


def parse_table_with_ai(table_text: str, model: str = 'qwen2.5:7b') -> Dict:
    """
    ä½¿ç”¨AIè§£æè¡¨æ ¼

    Args:
        table_text: è¡¨æ ¼çš„Markdownæ–‡æœ¬
        model: ä½¿ç”¨çš„AIæ¨¡å‹

    Returns:
        è§£æåçš„ç»“æ„åŒ–æ•°æ®
    """
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨æ ¼è§£æå’Œæ–‡æœ¬æ¸…ç†åŠ©æ‰‹ã€‚

ä»»åŠ¡ï¼šè§£æè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»è¡¨æ ¼ï¼Œå¹¶æ¸…ç†æ–‡æœ¬æ ¼å¼é—®é¢˜ã€‚

è§£æè§„åˆ™ï¼š
1. æå–è¡¨æ ¼æ ‡é¢˜ï¼ˆå¦‚"è¡¨1 è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»"ï¼‰
2. æå–ä¸“ä¸šåç§°ï¼ˆä»æ ‡é¢˜ä¸­æå–ï¼Œå»æ‰"ä¸“ä¸š"äºŒå­—ï¼Œä½†å¿…é¡»ä¿ç•™æ‹¬å·åŠæ‹¬å·å†…çš„æ‰€æœ‰å†…å®¹ï¼‰
   - ä¾‹å¦‚ï¼š"è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ä¸“ä¸š" â†’ "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯"
   - ä¾‹å¦‚ï¼š"è½¯ä»¶å·¥ç¨‹ä¸“ä¸š" â†’ "è½¯ä»¶å·¥ç¨‹"
   - ä¾‹å¦‚ï¼š"è®¡ç®—æœºç§‘å­¦è‹±æ‰ç­ï¼ˆè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼‰ä¸“ä¸š" â†’ "è®¡ç®—æœºç§‘å­¦è‹±æ‰ç­ï¼ˆè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼‰"
   - ä¾‹å¦‚ï¼š"è½¯ä»¶å·¥ç¨‹ï¼ˆå“è¶Šå·¥ç¨‹å¸ˆç­ï¼‰ä¸“ä¸š" â†’ "è½¯ä»¶å·¥ç¨‹ï¼ˆå“è¶Šå·¥ç¨‹å¸ˆç­ï¼‰"
   - ä¾‹å¦‚ï¼š"è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼ˆå›½é™…ç­ï¼‰ä¸“ä¸š" â†’ "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼ˆå›½é™…ç­ï¼‰"
   - é‡è¦ï¼šæ‹¬å·åŠæ‹¬å·å†…çš„å†…å®¹æ˜¯ä¸“ä¸šåç§°çš„ä¸€éƒ¨åˆ†ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™ï¼Œä¸è¦åˆ é™¤æˆ–ç®€åŒ–
3. è§£æè¡¨æ ¼å†…å®¹ï¼Œæå–æ¯ä¸€è¡Œçš„ï¼š
   - æ¯•ä¸šè¦æ±‚ç¼–å·ï¼ˆå¦‚"1"ã€"2"ã€"6"ç­‰ï¼Œä»"æ¯•ä¸šè¦æ±‚1ï¼š"æˆ–"1.å·¥ç¨‹çŸ¥è¯†"ä¸­æå–ï¼‰
   - æ¯•ä¸šè¦æ±‚å†…å®¹ï¼ˆç¬¬1åˆ—ï¼Œå®Œæ•´å†…å®¹ï¼‰
   - æŒ‡æ ‡ç‚¹ï¼ˆç¬¬2åˆ—ï¼‰
   - è¯¾ç¨‹ç›®æ ‡ï¼ˆç¬¬3åˆ—ï¼‰

æ–‡æœ¬æ¸…ç†è§„åˆ™ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
1. åˆ é™¤æ‰€æœ‰<br>æ ‡ç­¾
2. æ™ºèƒ½åˆå¹¶è¢«<br>æ‰“æ–­çš„å†…å®¹ï¼š
   - "1<br>-<br>1" â†’ "1-1"
   - "1 1" â†’ "1-1"ï¼ˆå¦‚æœæ˜¯æŒ‡æ ‡ç‚¹ç¼–å·ï¼‰
   - "è‡ªç„¶ç§‘<br>å­¦" â†’ "è‡ªç„¶ç§‘å­¦"
   - "å·¥ç¨‹é—®<br>é¢˜" â†’ "å·¥ç¨‹é—®é¢˜"
3. åˆ é™¤è¯è¯­ä¸­é—´ä¸è‡ªç„¶çš„ç©ºæ ¼ï¼š
   - "åŸºç¡€ çŸ¥è¯†" â†’ "åŸºç¡€çŸ¥è¯†"
   - "å…¸ å‹ç¯èŠ‚" â†’ "å…¸å‹ç¯èŠ‚"
   - "é€»è¾‘æ€ç»´åˆ†ææ–¹æ³• ç”¨äº" â†’ "é€»è¾‘æ€ç»´åˆ†ææ–¹æ³•ç”¨äº"
4. ä¿®æ­£æ•°å­—æ ¼å¼ï¼š
   - "0 .5" â†’ "0.5"
   - "0. 4" â†’ "0.4"
5. ä¿ç•™æœ‰æ„ä¹‰çš„ç©ºæ ¼ï¼š
   - "ç›®æ ‡1ï¼š0.5 ç›®æ ‡2ï¼š0.5" ä¸­çš„ç©ºæ ¼ä¿ç•™
   - å¥å­ä¹‹é—´çš„ç©ºæ ¼ä¿ç•™
6. åˆ é™¤å¤šä½™çš„æ ‡ç‚¹ç¬¦å·ï¼š
   - "ã€‚<br>ã€<br>" â†’ ""
   - "ã€<br>ï¼Œ<br>" â†’ "ã€"

è¾“å‡ºæ ¼å¼ï¼š
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼š
{
  "table_title": "è¡¨æ ¼æ ‡é¢˜",
  "major": "ä¸“ä¸šåç§°ï¼ˆä¸å«'ä¸“ä¸š'äºŒå­—ï¼‰",
  "mappings": [
    {
      "requirement_number": "æ¯•ä¸šè¦æ±‚ç¼–å·ï¼ˆå¦‚'1'ã€'2'ï¼‰",
      "requirement": "æ¯•ä¸šè¦æ±‚å†…å®¹",
      "indicator": "æŒ‡æ ‡ç‚¹å†…å®¹",
      "course_goals": "è¯¾ç¨‹ç›®æ ‡å†…å®¹"
    }
  ]
}

ç¤ºä¾‹ï¼š
è¾“å…¥ï¼š"æ¯•ä¸šè¦æ±‚ 1 å·¥ç¨‹çŸ¥è¯† ï¼šèƒ½å¤Ÿå°†æ•°å­¦ä¸è‡ªç„¶ç§‘<br>å­¦ å·¥ç¨‹å­¦ç§‘..."
è¾“å‡ºï¼š{"requirement_number": "1", "requirement": "æ¯•ä¸šè¦æ±‚1å·¥ç¨‹çŸ¥è¯†ï¼šèƒ½å¤Ÿå°†æ•°å­¦ä¸è‡ªç„¶ç§‘å­¦å·¥ç¨‹å­¦ç§‘..."}

è¾“å…¥ï¼š"1 1 èƒ½å¤Ÿå°†æ•°å­¦å’Œè‡ªç„¶ç§‘å­¦çš„åŸºç¡€<br>çŸ¥è¯† é€»è¾‘æ€ç»´..."
è¾“å‡ºï¼š{"indicator": "1-1 èƒ½å¤Ÿå°†æ•°å­¦å’Œè‡ªç„¶ç§‘å­¦çš„åŸºç¡€çŸ¥è¯†é€»è¾‘æ€ç»´..."}

è¾“å…¥ï¼š"ç›®æ ‡1ï¼š0 .5<br>ç›®æ ‡2ï¼š0 .5"
è¾“å‡ºï¼š{"course_goals": "ç›®æ ‡1ï¼š0.5 ç›®æ ‡2ï¼š0.5"}
"""
    
    user_prompt = f"""è¯·è§£æä»¥ä¸‹è¡¨æ ¼ï¼š

{table_text}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"""
    
    try:
        response = ollama.chat(
            model=model,
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ],
            options={
                'temperature': 0.1,
            }
        )
        
        result_text = response['message']['content'].strip()
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        result_text = re.sub(r'```json\s*', '', result_text)
        result_text = re.sub(r'```\s*', '', result_text)
        
        result = json.loads(result_text)
        return result
        
    except json.JSONDecodeError as e:
        print(f"         âš ï¸  JSONè§£æå¤±è´¥")
        return {
            'table_title': 'è§£æå¤±è´¥',
            'major': 'æœªçŸ¥',
            'mappings': [],
            'raw_text': table_text[:500],
            'error': str(e)
        }
    except Exception as e:
        print(f"         âš ï¸  AIè§£æå¤±è´¥: {e}")
        return {
            'table_title': 'è§£æå¤±è´¥',
            'major': 'æœªçŸ¥',
            'mappings': [],
            'raw_text': table_text[:500],
            'error': str(e)
        }


# ==================== ä»»åŠ¡4: æå–è¯¾ç¨‹è”ç³» (ä»£ç å®šä½+AIæ¸…ç†) ====================

def extract_course_relations_raw(md_content: str) -> Optional[str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å®šä½"ä¸å…¶å®ƒè¯¾ç¨‹çš„è”ç³»"ç« èŠ‚

    Args:
        md_content: Markdownæ–‡ä»¶å†…å®¹

    Returns:
        åŸå§‹ç« èŠ‚æ–‡æœ¬ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    # åŒ¹é…ç« èŠ‚æ ‡é¢˜ï¼šå¯èƒ½æ˜¯"ä¸‰ã€"ã€"å››ã€"ã€"äº”ã€"ç­‰
    # åŒ¹é…åˆ°ä¸‹ä¸€ä¸ªç« èŠ‚æ ‡é¢˜ä¸ºæ­¢
    pattern = r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€?\s*ä¸å…¶[å®ƒä»–]è¯¾ç¨‹çš„è”ç³».*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|\Z)'

    match = re.search(pattern, md_content, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()

    return None


def clean_course_relations_with_ai(raw_text: str, model: str = 'qwen2.5:7b') -> Dict:
    """
    ä½¿ç”¨AIæ¸…ç†å’Œç»“æ„åŒ–è¯¾ç¨‹è”ç³»ä¿¡æ¯

    Args:
        raw_text: åŸå§‹ç« èŠ‚æ–‡æœ¬
        model: ä½¿ç”¨çš„AIæ¨¡å‹

    Returns:
        ç»“æ„åŒ–çš„è¯¾ç¨‹è”ç³»æ•°æ®
    """
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ¸…ç†å’Œç»“æ„åŒ–åŠ©æ‰‹ã€‚

ä»»åŠ¡ï¼šæå–å’Œæ¸…ç†"ä¸å…¶å®ƒè¯¾ç¨‹çš„è”ç³»"ç« èŠ‚ä¸­çš„å…ˆä¿®è¯¾ç¨‹å’Œåç»­è¯¾ç¨‹ä¿¡æ¯ã€‚

æå–è§„åˆ™ï¼š
1. è¯†åˆ«"å…ˆä¿®è¯¾ç¨‹"ï¼ˆä¹Ÿå¯èƒ½æ˜¯"å…ˆä¿®è¯¾"ã€"å‰ç½®è¯¾ç¨‹"ç­‰ï¼‰
2. è¯†åˆ«"åç»­è¯¾ç¨‹"ï¼ˆä¹Ÿå¯èƒ½æ˜¯"åä¿®è¯¾ç¨‹"ã€"åç»­è¯¾"ç­‰ï¼‰
3. æå–è¯¾ç¨‹åç§°åˆ—è¡¨

æ–‡æœ¬æ¸…ç†è§„åˆ™ï¼š
1. åˆ é™¤æ‰€æœ‰<br>æ ‡ç­¾
2. åˆ é™¤é¡µç å’Œå¤šä½™ç©ºè¡Œ
3. åˆ é™¤ä¸è‡ªç„¶çš„ç©ºæ ¼ï¼š
   - "ç¨‹åº è®¾è®¡" â†’ "ç¨‹åºè®¾è®¡"
   - "æ•°æ® ç»“æ„" â†’ "æ•°æ®ç»“æ„"
4. ç»Ÿä¸€åˆ†éš”ç¬¦ï¼š
   - å°†"ã€"ã€"ï¼Œ"ã€"ï¼›"ç»Ÿä¸€ä¸º"ã€"
5. åˆ é™¤"æ— "ã€"æ— ã€‚"ç­‰æ— æ„ä¹‰å†…å®¹
6. å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å…ˆä¿®/åç»­è¯¾ç¨‹æ ‡ç­¾ï¼Œä½†æœ‰æè¿°æ€§æ–‡å­—ï¼Œæå–åˆ°descriptionå­—æ®µ

è¾“å‡ºæ ¼å¼ï¼š
ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼š
{
  "prerequisite_courses": ["è¯¾ç¨‹1", "è¯¾ç¨‹2"],
  "subsequent_courses": ["è¯¾ç¨‹1", "è¯¾ç¨‹2"],
  "description": "å…¶ä»–æè¿°æ€§æ–‡å­—ï¼ˆå¦‚æœæœ‰ï¼‰"
}

ç¤ºä¾‹1ï¼š
è¾“å…¥ï¼š
å…ˆä¿®è¯¾ç¨‹ï¼š ç¨‹åºè®¾è®¡åŸºç¡€ã€ç¦»æ•£æ•°å­¦ã€‚
åç»­è¯¾ç¨‹ï¼š æ•°æ®åº“ç³»ç»Ÿã€æ“ä½œç³»ç»Ÿã€ç¼–è¯‘åŸç†ã€‚

è¾“å‡ºï¼š
{
  "prerequisite_courses": ["ç¨‹åºè®¾è®¡åŸºç¡€", "ç¦»æ•£æ•°å­¦"],
  "subsequent_courses": ["æ•°æ®åº“ç³»ç»Ÿ", "æ“ä½œç³»ç»Ÿ", "ç¼–è¯‘åŸç†"],
  "description": ""
}

ç¤ºä¾‹2ï¼š
è¾“å…¥ï¼š
å…ˆä¿®è¯¾ç¨‹ï¼š æ— ï¼›
åç»­è¯¾ç¨‹ï¼š é¢å‘å¯¹è±¡ç¨‹åºè®¾è®¡ï¼ˆC++ï¼‰ã€é¢å‘å¯¹è±¡ç¨‹åºè®¾è®¡ï¼ˆJavaï¼‰ã€‚

è¾“å‡ºï¼š
{
  "prerequisite_courses": [],
  "subsequent_courses": ["é¢å‘å¯¹è±¡ç¨‹åºè®¾è®¡ï¼ˆC++ï¼‰", "é¢å‘å¯¹è±¡ç¨‹åºè®¾è®¡ï¼ˆJavaï¼‰"],
  "description": ""
}

ç¤ºä¾‹3ï¼š
è¾“å…¥ï¼š
æœ¬è¯¾ç¨‹ä¸ä¸­å›½è¿‘ç°ä»£å²çº²è¦ï¼Œæ€æƒ³é“å¾·ä¿®å…»ä¸æ³•å¾‹åŸºç¡€ï¼Œæ¯›æ³½ä¸œæ€æƒ³å’Œä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰ç†è®ºä½“ç³»æ¦‚è®ºä»¥åŠé©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†æ¦‚è®ºè¯¾ç¨‹äº’ä¸ºæ”¯æ’‘ï¼Œå…±åŒåä½œï¼Œä»¥è¾¾æˆæ•™å­¦ç›®æ ‡çš„è¦æ±‚ã€‚

è¾“å‡ºï¼š
{
  "prerequisite_courses": [],
  "subsequent_courses": [],
  "description": "æœ¬è¯¾ç¨‹ä¸ä¸­å›½è¿‘ç°ä»£å²çº²è¦ã€æ€æƒ³é“å¾·ä¿®å…»ä¸æ³•å¾‹åŸºç¡€ã€æ¯›æ³½ä¸œæ€æƒ³å’Œä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰ç†è®ºä½“ç³»æ¦‚è®ºä»¥åŠé©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†æ¦‚è®ºè¯¾ç¨‹äº’ä¸ºæ”¯æ’‘ï¼Œå…±åŒåä½œï¼Œä»¥è¾¾æˆæ•™å­¦ç›®æ ‡çš„è¦æ±‚ã€‚"
}
"""

    user_prompt = f"""è¯·æå–å’Œæ¸…ç†ä»¥ä¸‹æ–‡æœ¬ä¸­çš„è¯¾ç¨‹è”ç³»ä¿¡æ¯ï¼š

{raw_text}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"""

    try:
        response = ollama.chat(
            model=model,
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ],
            options={
                'temperature': 0.1,
            }
        )

        result_text = response['message']['content'].strip()

        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        result_text = re.sub(r'```json\s*', '', result_text)
        result_text = re.sub(r'```\s*', '', result_text)

        result = json.loads(result_text)
        return result

    except json.JSONDecodeError as e:
        print(f"         âš ï¸  JSONè§£æå¤±è´¥")
        return {
            "prerequisite_courses": [],
            "subsequent_courses": [],
            "description": raw_text[:200]  # ä¿ç•™å‰200å­—ç¬¦ä½œä¸ºå¤‡ä»½
        }
    except Exception as e:
        print(f"         âš ï¸  AIå¤„ç†å¤±è´¥: {str(e)}")
        return {
            "prerequisite_courses": [],
            "subsequent_courses": [],
            "description": ""
        }


# ==================== ä¸¤çº§å¤„ç†ç­–ç•¥ ====================
# Level 1: è¡¨æ ¼æå– + é¢„å¤„ç† + AIè§£æï¼ˆæœ¬åœ°Ollama qwen2.5:7bï¼‰
# Level 2: æ•´ç« å¤„ç† + DeepSeekå¢å¼ºç‰ˆpromptï¼ˆDeepSeek APIï¼‰

def is_parsing_failed(mappings: List[Dict]) -> bool:
    """
    åˆ¤æ–­è§£ææ˜¯å¦å¤±è´¥

    Args:
        mappings: requirement_mappingsåˆ—è¡¨

    Returns:
        Trueè¡¨ç¤ºè§£æå¤±è´¥ï¼Œéœ€è¦é‡è¯•
    """
    # æ¡ä»¶1ï¼šæ²¡æœ‰æå–åˆ°ä»»ä½•è¡¨æ ¼
    if len(mappings) == 0:
        return True

    # æ¡ä»¶2ï¼šæ‰€æœ‰è¡¨æ ¼çš„mappingséƒ½ä¸ºç©º
    if all(len(table.get('mappings', [])) == 0 for table in mappings):
        return True

    # æ¡ä»¶3ï¼šæœ‰"è§£æå¤±è´¥"æˆ–"error"çš„è¡¨æ ¼
    if any('error' in table or table.get('table_title') == 'è§£æå¤±è´¥' for table in mappings):
        return True

    # æ¡ä»¶4ï¼šè¡¨æ ¼æ•°é‡å¼‚å¸¸å°‘ï¼ˆå°‘äº2ä¸ªå¯èƒ½æœ‰é—®é¢˜ï¼‰
    valid_tables = [t for t in mappings if len(t.get('mappings', [])) > 0]
    if len(valid_tables) < 2:
        return True

    return False


def parse_full_chapter_with_deepseek(chapter_text: str) -> List[Dict]:
    """
    Level 2: ä½¿ç”¨DeepSeek APIå¤„ç†æ•´ä¸ªç¬¬äºŒç« ï¼ˆå¢å¼ºç‰ˆpromptï¼ŒåŒ…å«è¯¦ç»†ç¤ºä¾‹ï¼‰

    Args:
        chapter_text: ç¬¬äºŒç« çš„å®Œæ•´æ–‡æœ¬

    Returns:
        è¡¨æ ¼åˆ—è¡¨
    """
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯¾ç¨‹å¤§çº²è§£æåŠ©æ‰‹ã€‚è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦ä½ ä»”ç»†å¤„ç†å„ç§æ ¼å¼é—®é¢˜ã€‚

ä»»åŠ¡ï¼šä»è¯¾ç¨‹æ•™å­¦å¤§çº²çš„ç¬¬äºŒç« ä¸­æå–æ‰€æœ‰ä¸“ä¸šçš„æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»è¡¨æ ¼ã€‚

=== å…³é”®æŒ‘æˆ˜å’Œè§£å†³æ–¹æ³• ===

1. è¡¨æ ¼æ ‡é¢˜è¯†åˆ«ï¼ˆæœ€é‡è¦ï¼ï¼‰ï¼š

   é”™è¯¯ç¤ºä¾‹1ï¼šæ ‡é¢˜åµŒå…¥åœ¨è¡¨æ ¼ç¬¬ä¸€è¡Œä¸”è¢«æˆªæ–­
   åŸå§‹æ–‡æœ¬ï¼š|è¡¨2 è½¯ä»¶å·¥ç¨‹ä¸“ä¸šè¯¾ç¨‹ç›®æ ‡|æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»|Col3|
   æ­£ç¡®å¤„ç†ï¼šåˆå¹¶ä¸º "è¡¨2 è½¯ä»¶å·¥ç¨‹ä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»"

   é”™è¯¯ç¤ºä¾‹2ï¼šæå–äº†æè¿°æ€§æ–‡å­—
   åŸå§‹æ–‡æœ¬ï¼šè¯¾ç¨‹ç›®æ ‡ä¸ç›¸å…³æ¯•ä¸šè¦æ±‚åŠå…¶æŒ‡æ ‡ç‚¹çš„å¯¹åº”å…³ç³»å¦‚è¡¨ 6 æ‰€ç¤ºã€‚
   é”™è¯¯è¾“å‡ºï¼štable_title: "è¯¾ç¨‹ç›®æ ‡ä¸ç›¸å…³æ¯•ä¸šè¦æ±‚åŠå…¶æŒ‡æ ‡ç‚¹çš„å¯¹åº”å…³ç³»å¦‚è¡¨ 6 æ‰€ç¤ºã€‚"
   æ­£ç¡®å¤„ç†ï¼šè¿™ä¸æ˜¯æ ‡é¢˜ï¼åº”è¯¥ç»§ç»­å¯»æ‰¾çœŸæ­£çš„è¡¨æ ¼æ ‡é¢˜ï¼ˆå¦‚ï¼šè¡¨6 æ™ºèƒ½åˆ¶é€ ï¼ˆæœºæ¢°ç±»ï¼‰ä¸“ä¸š...ï¼‰

   æ ‡å‡†æ ¼å¼ï¼šè¡¨X XXXä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»

2. ä¸“ä¸šåç§°æå–ï¼ˆå¿…é¡»ä¸æ ‡é¢˜ä¸€è‡´ï¼ï¼‰ï¼š

   ç¤ºä¾‹1ï¼š
   æ ‡é¢˜ï¼šè¡¨2 è½¯ä»¶å·¥ç¨‹ä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»
   æ­£ç¡®ï¼šmajor: "è½¯ä»¶å·¥ç¨‹"
   é”™è¯¯ï¼šmajor: "æ™ºèƒ½è®¡ç®—ä¸æ•°æ®ç§‘å­¦"ï¼ˆè¿™æ˜¯å¦ä¸€ä¸ªè¡¨æ ¼çš„ä¸“ä¸šï¼‰

   ç¤ºä¾‹2ï¼š
   æ ‡é¢˜ï¼šè¡¨6 æ™ºèƒ½åˆ¶é€ ï¼ˆæœºæ¢°ç±»ï¼‰ä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»
   æ­£ç¡®ï¼šmajor: "æ™ºèƒ½åˆ¶é€ ï¼ˆæœºæ¢°ç±»ï¼‰"
   é”™è¯¯ï¼šmajor: "äººå·¥æ™ºèƒ½å®‰å…¨ï¼ˆç½‘ç»œç©ºé—´å®‰å…¨ç±»ï¼‰"ï¼ˆè¿™æ˜¯å¦ä¸€ä¸ªè¡¨æ ¼çš„ä¸“ä¸šï¼‰

   ç¤ºä¾‹3ï¼š
   æ ‡é¢˜ï¼šè¡¨3 æ™ºèƒ½è®¡ç®—ä¸æ•°æ®ç§‘å­¦ï¼ˆè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼‰ä¸“ä¸š...
   æ­£ç¡®ï¼šmajor: "æ™ºèƒ½è®¡ç®—ä¸æ•°æ®ç§‘å­¦ï¼ˆè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ï¼‰"

   éªŒè¯æ–¹æ³•ï¼šæ£€æŸ¥æ•°æ®å†…å®¹ä¸­çš„ä¸“ä¸šæè¿°æ˜¯å¦ä¸majorä¸€è‡´

3. è¡¨æ ¼å†…å®¹å¤„ç†ï¼š

   - è·¨é¡µåˆ†å‰²çš„è¡¨æ ¼è¡Œéœ€è¦åˆå¹¶
   - æ ¼å¼é”™è¯¯ä¿®å¤ï¼š
     * "3 1" â†’ "3-1"
     * "1.1" æˆ– "1-2" éƒ½æ˜¯æœ‰æ•ˆçš„requirement_numberæ ¼å¼
     * "ç›®æ ‡5 03" â†’ "ç›®æ ‡5ï¼š0.3"
     * "ç›®æ ‡3 06 ç›®æ ‡4 04" â†’ "ç›®æ ‡3ï¼š0.6 ç›®æ ‡4ï¼š0.4"
     * "0 .7" â†’ "0.7"
     * "æ¯•ä¸šè¦æ±‚1ï¼šå·¥ç¨‹çŸ¥è¯†ï¼š..." â†’ requirement_number: "1"
     * "æ¯•ä¸šè¦æ±‚1-2ï¼š..." â†’ requirement_number: "1-2"

4. æ•°æ®ä¸€è‡´æ€§éªŒè¯ï¼ˆå…³é”®ï¼ï¼‰ï¼š

   æ£€æŸ¥ç‚¹ï¼š
   - table_titleä¸­çš„ä¸“ä¸šåç§° = majorå­—æ®µ
   - majorå­—æ®µ = mappingsä¸­çš„ä¸“ä¸šæè¿°

   ç¤ºä¾‹ï¼š
   å¦‚æœtable_titleæ˜¯"è¡¨2 è½¯ä»¶å·¥ç¨‹ä¸“ä¸š..."
   é‚£ä¹ˆmajorå¿…é¡»æ˜¯"è½¯ä»¶å·¥ç¨‹"
   å¹¶ä¸”mappingsä¸­çš„requirementåº”è¯¥æåˆ°"è½¯ä»¶å·¥ç¨‹"ï¼ˆä¸æ˜¯"æ™ºèƒ½è®¡ç®—"æˆ–å…¶ä»–ä¸“ä¸šï¼‰

=== è¾“å‡ºæ ¼å¼ ===

JSONæ•°ç»„ï¼ŒæŒ‰ç…§è¡¨æ ¼åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¡ºåºï¼ˆè¡¨1ã€è¡¨2ã€è¡¨3...ï¼‰ï¼š

[
  {
    "table_title": "è¡¨1 è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»",
    "major": "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯",
    "mappings": [
      {
        "requirement_number": "1-2",
        "requirement": "å·¥ç¨‹çŸ¥è¯†ï¼šæŒæ¡æ•°å­¦ã€è‡ªç„¶ç§‘å­¦ã€å·¥ç¨‹åŸºç¡€ã€è®¡ç®—æœºä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†...",
        "indicator": "æŒæ¡è®¡ç®—æœºç§‘å­¦æ ¸å¿ƒçŸ¥è¯†ä¸ç†è®ºï¼Œèƒ½å¤Ÿé’ˆå¯¹è®¡ç®—æœºé¢†åŸŸå¤æ‚å·¥ç¨‹é—®é¢˜å»ºç«‹æ¨¡å‹...",
        "course_goals": "ç›®æ ‡1ï¼š0.6 ç›®æ ‡2ï¼š0.4"
      }
    ]
  },
  {
    "table_title": "è¡¨2 è½¯ä»¶å·¥ç¨‹ä¸“ä¸šè¯¾ç¨‹ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»",
    "major": "è½¯ä»¶å·¥ç¨‹",
    "mappings": [...]
  }
]

=== é‡è¦æé†’ ===

- åªè¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å†…å®¹
- ç¡®ä¿æ¯ä¸ªè¡¨æ ¼éƒ½æœ‰å®Œæ•´çš„mappings
- å¦‚æœæŸä¸ªè¡¨æ ¼æ— æ³•è§£æï¼Œè·³è¿‡å®ƒ
- ä»”ç»†æ ¸å¯¹table_titleå’Œmajorçš„ä¸€è‡´æ€§
- æŒ‰ç…§è¡¨æ ¼ç¼–å·é¡ºåºè¾“å‡ºï¼ˆè¡¨1ã€è¡¨2ã€è¡¨3...ï¼‰
"""

    user_prompt = f"""è¯·ä»ä»¥ä¸‹ç¬¬äºŒç« å†…å®¹ä¸­æå–æ‰€æœ‰ä¸“ä¸šçš„æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»è¡¨æ ¼ã€‚

ç‰¹åˆ«æ³¨æ„ï¼š
1. ä»”ç»†è¯†åˆ«æ¯ä¸ªè¡¨æ ¼çš„çœŸå®æ ‡é¢˜ï¼ˆä¸è¦è¢«æè¿°æ€§æ–‡å­—è¯¯å¯¼ï¼‰
2. ç¡®ä¿table_titleå’Œmajorå­—æ®µä¸€è‡´
3. æŒ‰ç…§è¡¨æ ¼ç¼–å·é¡ºåºè¾“å‡º

{chapter_text}

åªè¾“å‡ºJSONæ•°ç»„ï¼Œæ ¼å¼å¦‚ä¸Šæ‰€è¿°ã€‚"""

    try:
        response = requests.post(
            DEEPSEEK_API_URL,
            headers={
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1
            },
            timeout=120
        )

        if response.status_code != 200:
            print(f"         âš ï¸  DeepSeek APIé”™è¯¯: {response.status_code}")
            return []

        result_json = response.json()
        result_text = result_json['choices'][0]['message']['content'].strip()

        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        result_text = re.sub(r'```json\s*', '', result_text)
        result_text = re.sub(r'```\s*', '', result_text)

        result = json.loads(result_text)

        # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨
        if isinstance(result, dict):
            result = [result]

        return result

    except Exception as e:
        print(f"         âš ï¸  Level 3å¤„ç†å¤±è´¥: {str(e)}")
        return []


# ==================== ä¸»å¤„ç†æµç¨‹ ====================

def process_single_file(md_file_path: str, model: str = 'qwen2.5:7b') -> Dict:
    """
    å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶
    
    Args:
        md_file_path: Markdownæ–‡ä»¶è·¯å¾„
        model: ä½¿ç”¨çš„AIæ¨¡å‹
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    # è¯»å–æ–‡ä»¶
    with open(md_file_path, 'r', encoding='utf-8') as f:
        md_content = f.read()
    
    result = {
        'file_name': os.path.basename(md_file_path),
        'processed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # ä»»åŠ¡1: æå–è¯¾ç¨‹åç§°å’Œä»£ç ï¼ˆä»£ç ä¼˜å…ˆ+AIè¾…åŠ©ï¼‰
    print("      [1/4] æå–è¯¾ç¨‹åç§°å’Œä»£ç ...")

    # å…ˆç”¨ä»£ç æå–
    course_name_regex = extract_course_name_with_regex(md_content)
    course_code_regex = extract_course_code_with_regex(md_content)

    # å¦‚æœä»£ç æå–å¤±è´¥ï¼Œä½¿ç”¨AI
    if not course_name_regex or not course_code_regex:
        print("           ä»£ç æå–ä¸å®Œæ•´ï¼Œä½¿ç”¨AIè¾…åŠ©...")
        course_info_ai = extract_course_basic_info_with_ai(md_content, model)
        result['course_name'] = course_name_regex or course_info_ai['course_name']
        result['course_code'] = course_code_regex or course_info_ai['course_code']
    else:
        result['course_name'] = course_name_regex
        result['course_code'] = course_code_regex

    # ä»»åŠ¡2: æå–å¹¶æ¸…ç†è¯¾ç¨‹ç›®æ ‡ï¼ˆä»£ç +AIï¼‰
    print("      [2/4] æå–è¯¾ç¨‹ç›®æ ‡...")
    chapter_1_raw = extract_chapter_1_raw(md_content)
    if chapter_1_raw:
        result['course_goals'] = clean_chapter_1_with_ai(chapter_1_raw, model)
    else:
        result['course_goals'] = "æœªæ‰¾åˆ°è¯¾ç¨‹ç›®æ ‡ç« èŠ‚"

    # ä»»åŠ¡3: æå–æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»ï¼ˆä¸‰çº§å¤„ç†ç­–ç•¥ï¼‰
    print("      [3/4] æå–æ¯•ä¸šè¦æ±‚å¯¹åº”å…³ç³»...")
    chapter_2_raw = extract_chapter_2_raw(md_content)

    if chapter_2_raw and validate_chapter_2_title(chapter_2_raw):
        # Level 1: å½“å‰æ–¹æ³•ï¼ˆè¡¨æ ¼æå– + é¢„å¤„ç† + AIï¼‰
        tables = extract_tables_raw(chapter_2_raw)
        print(f"           Level 1: æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")

        mappings = []
        for i, table_text in enumerate(tables, 1):
            print(f"           è§£æè¡¨æ ¼ {i}/{len(tables)}...", end=' ')
            # é¢„å¤„ç†è¡¨æ ¼ï¼Œä¿®å¤å¼‚å¸¸æ ¼å¼
            preprocessed_table = preprocess_table(table_text)
            parsed = parse_table_with_ai(preprocessed_table, model)
            mappings.append(parsed)
            print("âœ…")

        result['requirement_mappings'] = mappings

        # æ£€æŸ¥Level 1æ˜¯å¦å¤±è´¥
        if is_parsing_failed(mappings):
            print("           âš ï¸  Level 1è§£æå¤±è´¥ï¼Œå°è¯•Level 2ï¼ˆæ•´ç« å¤„ç†-DeepSeekï¼‰...")

            # Level 2: æ•´ç« å¤„ç†ï¼ˆDeepSeekå¢å¼ºç‰ˆpromptï¼‰
            mappings_level2 = parse_full_chapter_with_deepseek(chapter_2_raw)

            if mappings_level2 and not is_parsing_failed(mappings_level2):
                print("           âœ… Level 2æˆåŠŸï¼")
                result['requirement_mappings'] = mappings_level2
            else:
                print("           âŒ Level 2ä»ç„¶å¤±è´¥ï¼Œä¿ç•™Level 1ç»“æœ")
                # ä¿ç•™Level 1çš„ç»“æœï¼ˆå³ä½¿å¤±è´¥ï¼‰
    else:
        result['requirement_mappings'] = []

    # ä»»åŠ¡4: æå–è¯¾ç¨‹è”ç³»ï¼ˆä»£ç +AIï¼‰
    print("      [4/4] æå–è¯¾ç¨‹è”ç³»...")
    course_relations_raw = extract_course_relations_raw(md_content)
    if course_relations_raw:
        result['course_relations'] = clean_course_relations_with_ai(course_relations_raw, model)
    else:
        result['course_relations'] = {
            "prerequisite_courses": [],
            "subsequent_courses": [],
            "description": ""
        }

    return result


def process_all_markdown_files(input_dir='docs/md', output_dir='docs/json', 
                                model='qwen2.5:7b', skip_existing=False):
    """
    æ‰¹é‡å¤„ç†æ‰€æœ‰Markdownæ–‡ä»¶
    
    Args:
        input_dir: è¾“å…¥ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        model: ä½¿ç”¨çš„AIæ¨¡å‹
        skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„JSONæ–‡ä»¶
    """
    if not OLLAMA_AVAILABLE:
        print("âŒ ollamaåº“æœªå®‰è£…ï¼Œæ— æ³•ç»§ç»­")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰Markdownæ–‡ä»¶
    md_files = sorted([f for f in os.listdir(input_dir) 
                      if f.endswith('.md') and f != 'è½¬æ¢è´¨é‡è¯„ä¼°.md'])
    
    print("="*70)
    print(f"ğŸ“š æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*70)
    
    success_count = 0
    failed_count = 0
    skipped_count = 0
    start_time = time.time()
    
    for i, md_file in enumerate(md_files, 1):
        md_path = os.path.join(input_dir, md_file)
        
        # è®¡ç®—è¿›åº¦å’Œæ—¶é—´
        elapsed = time.time() - start_time
        if i > 1:
            avg_time = elapsed / (i - 1)
            eta = avg_time * (len(md_files) - i + 1)
            eta_str = time.strftime('%H:%M:%S', time.gmtime(eta))
        else:
            eta_str = "è®¡ç®—ä¸­..."
        
        elapsed_str = time.strftime('%H:%M:%S', time.gmtime(elapsed))
        
        print("\n" + "="*70)
        print(f"ğŸ“„ [{i}/{len(md_files)}] {md_file}")
        print(f"â±ï¸  å·²ç”¨æ—¶: {elapsed_str} | é¢„è®¡å‰©ä½™: {eta_str}")
        print("="*70)
        
        try:
            # å¤„ç†æ–‡ä»¶
            result = process_single_file(md_path, model)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            course_code = result.get('course_code', 'æœªæ‰¾åˆ°')
            course_name = result.get('course_name', 'æœªæ‰¾åˆ°')
            output_filename = f"{course_code}_{course_name}.json"
            output_path = os.path.join(output_dir, output_filename)
            
            # æ£€æŸ¥æ˜¯å¦è·³è¿‡
            if skip_existing and os.path.exists(output_path):
                print(f"      â­ï¸  è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {output_filename}")
                skipped_count += 1
                continue
            
            # ä¿å­˜ç»“æœ
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"      âœ… æˆåŠŸä¿å­˜: {output_filename}")
            success_count += 1
            
        except Exception as e:
            print(f"      âŒ å¤„ç†å¤±è´¥: {e}")
            failed_count += 1
    
    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    total_time_str = time.strftime('%H:%M:%S', time.gmtime(total_time))
    avg_time = total_time / len(md_files) if md_files else 0
    
    print("\n" + "="*70)
    print("ğŸ‰ å¤„ç†å®Œæˆ!")
    print("="*70)
    print(f"  âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
    print(f"  âŒ å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
    print(f"  â­ï¸  è·³è¿‡: {skipped_count} ä¸ªæ–‡ä»¶")
    print(f"  â±ï¸  æ€»è€—æ—¶: {total_time_str}")
    print(f"  ğŸ“Š å¹³å‡é€Ÿåº¦: {avg_time:.2f}ç§’/æ–‡ä»¶")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("="*70)


if __name__ == '__main__':
    # æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶
    process_all_markdown_files(
        input_dir='docs/md',
        output_dir='docs/json',  # ä½¿ç”¨æ–°ç›®å½•é¿å…è¦†ç›–
        model='qwen2.5:7b',
        skip_existing=False
    )

