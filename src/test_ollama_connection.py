"""
æµ‹è¯•Ollamaè¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§

ä½¿ç”¨æ–¹æ³•:
    python src/test_ollama_connection.py
"""

import sys

def test_ollama_import():
    """æµ‹è¯•ollamaåº“æ˜¯å¦å®‰è£…"""
    print("1ï¸âƒ£ æµ‹è¯•ollamaåº“...")
    try:
        import ollama
        print("   âœ… ollamaåº“å·²å®‰è£…")
        return True
    except ImportError:
        print("   âŒ ollamaåº“æœªå®‰è£…")
        print("   è¯·è¿è¡Œ: pip install ollama")
        return False


def test_ollama_connection():
    """æµ‹è¯•OllamaæœåŠ¡è¿æ¥"""
    print("\n2ï¸âƒ£ æµ‹è¯•OllamaæœåŠ¡è¿æ¥...")
    try:
        import ollama
        # å°è¯•åˆ—å‡ºæ¨¡å‹
        models = ollama.list()
        print("   âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸")
        return True, models
    except Exception as e:
        print(f"   âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")
        return False, None


def test_model_availability(models_info):
    """æµ‹è¯•qwen2.5:7bæ¨¡å‹æ˜¯å¦å¯ç”¨"""
    print("\n3ï¸âƒ£ æ£€æŸ¥qwen2.5:7bæ¨¡å‹...")
    
    if not models_info:
        print("   âŒ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨")
        return False
    
    # æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
    # Ollamaè¿”å›çš„æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œéœ€è¦è®¿é—®modelså±æ€§
    try:
        # å°è¯•è®¿é—®modelså±æ€§ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
        if hasattr(models_info, 'models'):
            models = models_info.models
        # æˆ–è€…ä½œä¸ºå­—å…¸è®¿é—®ï¼ˆæ—§ç‰ˆæœ¬ï¼‰
        elif isinstance(models_info, dict):
            models = models_info.get('models', [])
        else:
            models = []

        # æå–æ¨¡å‹åç§°
        model_names = []
        for model in models:
            if hasattr(model, 'model'):
                model_names.append(model.model)
            elif isinstance(model, dict):
                model_names.append(model.get('name', model.get('model', '')))
            else:
                model_names.append(str(model))

        print(f"   å·²å®‰è£…çš„æ¨¡å‹: {len(model_names)} ä¸ª")
        for name in model_names:
            print(f"     - {name}")

        # æ£€æŸ¥qwen2.5:7b
        target_models = ['qwen2.5:7b', 'qwen2.5:latest']
        found = any(model in model_names for model in target_models)

        if found:
            print("   âœ… qwen2.5:7bæ¨¡å‹å·²å®‰è£…")
            return True
        else:
            print("   âŒ qwen2.5:7bæ¨¡å‹æœªå®‰è£…")
            print("   è¯·è¿è¡Œ: ollama pull qwen2.5:7b")
            return False
    except Exception as e:
        print(f"   âŒ è§£ææ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return False


def test_model_inference():
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print("\n4ï¸âƒ£ æµ‹è¯•æ¨¡å‹æ¨ç†...")
    try:
        import ollama
        import time
        
        start_time = time.time()
        
        response = ollama.chat(
            model='qwen2.5:7b',
            messages=[
                {'role': 'user', 'content': 'ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±'}
            ]
        )
        
        elapsed = time.time() - start_time
        
        print("   âœ… æ¨¡å‹æ¨ç†æˆåŠŸ")
        print(f"   â±ï¸  æ¨ç†è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"   ğŸ’¬ æ¨¡å‹å›å¤: {response['message']['content'][:100]}...")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        return False


def test_chinese_processing():
    """æµ‹è¯•ä¸­æ–‡æ–‡æœ¬å¤„ç†èƒ½åŠ›"""
    print("\n5ï¸âƒ£ æµ‹è¯•ä¸­æ–‡æ–‡æœ¬å¤„ç†...")
    try:
        import ollama
        
        test_text = """ä¸€ã€ è¯¾ç¨‹ç›®æ ‡


ã€Šç¨‹åºè®¾è®¡åŸºç¡€ã€‹æ˜¯è®¡ç®—æœºç±»ç›¸å…³ä¸“ä¸šçš„ä¸€é—¨é‡è¦çš„å­¦ç§‘åŸºç¡€è¯¾ç¨‹ï¼Œå®ƒä¸ºå…¶å®ƒä¸“ä¸šè¯¾ç¨‹å¥ å®šç¨‹


åºè®¾è®¡çš„åŸºç¡€ï¼Œåˆæ˜¯å…¶å®ƒä¸“ä¸šè¯¾ç¨‹çš„ç¨‹åºè®¾è®¡å·¥å…·ã€‚

534

è¯¾ç¨‹ç›®æ ‡ 1 ï¼šç³»ç»ŸæŒæ¡ C è¯­è¨€æ•°æ®ç±»å‹ã€å¸¸é‡ã€å˜é‡ã€è¿ç®—ç¬¦ã€è¡¨è¾¾å¼ã€è¯­å¥å’Œå‡½æ•°ç­‰è¯­ä¹‰ã€


è¯­æ³•å’Œä½¿ç”¨æ–¹æ³•ã€‚"""
        
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬æ¸…ç†åŠ©æ‰‹ã€‚è¯·åˆ é™¤æ–‡æœ¬ä¸­çš„é¡µç ï¼ˆå¦‚534ï¼‰å’Œå¤šä½™çš„ç©ºè¡Œï¼Œå°†å¥å­åˆå¹¶æˆè¿ç»­æ®µè½ã€‚åªè¾“å‡ºæ¸…ç†åçš„æ–‡æœ¬ã€‚"
        
        response = ollama.chat(
            model='qwen2.5:7b',
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': f"è¯·æ¸…ç†ä»¥ä¸‹æ–‡æœ¬ï¼š\n\n{test_text}"}
            ],
            options={'temperature': 0.3}
        )
        
        result = response['message']['content']
        
        print("   âœ… ä¸­æ–‡å¤„ç†æˆåŠŸ")
        print("   ğŸ“ æ¸…ç†ç»“æœ:")
        print("   " + "-"*66)
        for line in result.split('\n')[:5]:
            print(f"   {line}")
        print("   " + "-"*66)
        
        return True
        
    except Exception as e:
        print(f"   âŒ ä¸­æ–‡å¤„ç†å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("="*70)
    print("ğŸ§ª Ollamaç¯å¢ƒæµ‹è¯•")
    print("="*70)
    
    # æµ‹è¯•1: åº“å®‰è£…
    if not test_ollama_import():
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢: è¯·å…ˆå®‰è£…ollamaåº“")
        sys.exit(1)
    
    # æµ‹è¯•2: æœåŠ¡è¿æ¥
    connected, models_info = test_ollama_connection()
    if not connected:
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢: è¯·å¯åŠ¨OllamaæœåŠ¡")
        sys.exit(1)
    
    # æµ‹è¯•3: æ¨¡å‹å¯ç”¨æ€§
    if not test_model_availability(models_info):
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢: è¯·å®‰è£…qwen2.5:7bæ¨¡å‹")
        sys.exit(1)
    
    # æµ‹è¯•4: æ¨¡å‹æ¨ç†
    if not test_model_inference():
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢: æ¨¡å‹æ¨ç†å¤±è´¥")
        sys.exit(1)
    
    # æµ‹è¯•5: ä¸­æ–‡å¤„ç†
    if not test_chinese_processing():
        print("\nâš ï¸ è­¦å‘Š: ä¸­æ–‡å¤„ç†æµ‹è¯•å¤±è´¥ï¼Œä½†å¯ä»¥ç»§ç»­")
    
    # å…¨éƒ¨é€šè¿‡
    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ç¯å¢ƒé…ç½®æ­£ç¡®!")
    print("="*70)
    print("\nâœ… ä½ ç°åœ¨å¯ä»¥è¿è¡Œ:")
    print("   python src/test_single_file.py      # æµ‹è¯•å•ä¸ªæ–‡ä»¶")
    print("   python src/process_markdown.py      # æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶")
    print("="*70)


if __name__ == '__main__':
    main()

